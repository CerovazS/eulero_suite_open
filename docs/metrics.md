# Metrics and Evaluation Pipeline

This document describes the evaluation utilities shipped with the repository.
The tooling is designed to reproduce the metrics used during internal
benchmarks while remaining flexible enough for ad-hoc experiments.

## Overview

Metric computation is orchestrated by `test_metrics/compute_all.sh`. The script
performs three stages:

1. **Waveform reconstruction** – runs
   `test_metrics/generate_test_preds.py` to decode a checkpoint into audio
   files for every item under the target directory.
2. **Spectral metrics** – executes `test_metrics/compute_spectral.py` to gather
   SI-SDR and multiresolution STFT losses.
3. **Perceptual metrics** – optionally runs
   `test_metrics/compute_cdpam.py` and `test_metrics/compute_fad.py` for CDPAM
   and Frechet Audio Distance.

The helper keeps the directory layout of the reference dataset, so downstream
scripts can rely on consistent file names when locating predictions.

## Prerequisites

- Two Python environments:
  - a *main* environment (typically the project `.venv`) with PyTorch,
    torchaudio, and repository dependencies;
  - a *metrics* environment containing CDPAM, FAD, and their
    respective dependencies.
- A checkpoint generated by the training pipeline that embeds the
  `inference_config` block.
- A directory containing the reference audio files, usually the validation or
  test partition of the Jamendo dataset.

You can set defaults for the environments and dataset inside
`compute_all.sh` to avoid re-specifying them on every call.

## Running the End-to-End Script

```bash
./test_metrics/compute_all.sh \
  --checkpoint checkpoints/norm_111_bis_epoch_024_rigenerated.ckpt \
  --output-dir runs/inference/my_eval \
  --metrics-env /path/to/metrics-venv
```

**Key flags**

- `--target-dir PATH` – dataset root. Defaults to the `DEFAULT_TARGET_DIR`
  constant inside the script.
- `--checkpoint PATH` – Lightning checkpoint to evaluate.
- `--output-dir PATH` – destination directory for reconstructed waveforms.
- `--extensions LIST` – optional comma-separated extension filter
  (for example `wav,mp3`). The filter is honoured by both reconstruction and
  metric steps.
- `--infer-device DEV` – device used by `generate_test_preds.py`.
- `--cdpam-device DEV` – device used by CDPAM. Defaults to `cuda:0`.
- `--cdpam-chunk INT` – optional chunk length (in samples) for CDPAM when
  processing long tracks.
- `--csv-dir PATH` – location where CSV summaries are written.
- `--skip-cdpam`, `--skip-fad` – disable the respective metrics if their
  dependencies are unavailable.

The script logs each stage and writes intermediate results to the output
folder. Spectral metrics and CDPAM scores are aggregated into CSV files inside
`<output-dir>/metrics/`.

## Stage Details

### `generate_test_preds.py`

- Loads audio clips from the target directory using torchaudio.
- Reconstructs each clip with `EuleroEncodeDecode`, preserving relative
  subdirectories inside the output directory.
- Enables TF32 paths on CUDA-capable hardware and avoids gradient tracking to
  minimise latency during inference.
- Accepts the same `--extensions` filter as the shell wrapper. When the flag is
  omitted, the default covers WAV, FLAC, MP3, OGG, and M4A files.

You can run the script manually:

```bash
uv run test_metrics/generate_test_preds.py \
  --model-checkpoint checkpoints/model.ckpt \
  --target-dir /path/to/audio \
  --output-dir /tmp/reconstructions
```

### `compute_spectral.py`

- Aligns predictions with references using cross-correlation.
- Reports mean SI-SDR (via TorchMetrics) and the multi-resolution STFT loss
  used during training.
- Writes per-file entries to CSV when `--csv_out` is provided.

Run standalone:

```bash
uv run test_metrics/compute_spectral.py \
  --target-dir /path/to/reference \
  --preds-dir /path/to/reconstructions \
  --extensions wav,mp3 \
  --csv_out metrics/spectral.csv
```

### `compute_cdpam.py`

- Loads reference and generated audio with the CDPAM package loader.
- Supports optional chunking to limit GPU memory usage on long tracks.
- Outputs mean CDPAM and per-file scores (CSV optional).

Manual execution:

```bash
. /path/to/metrics-venv/bin/activate
python test_metrics/compute_cdpam.py \
  --target-dir /path/to/reference \
  --preds-dir /path/to/reconstructions \
  --device cuda:0 \
  --csv_out metrics/cdpam.csv
```

### `compute_fad.py`

- Wraps the `FrechetAudioDistance` package (CLAP backend) to compute FAD.
- Requires the metrics environment because of additional dependencies.
- Writes a plain-text report when launched via `compute_all.sh`.

Example:

```bash
. /path/to/metrics-venv/bin/activate
python test_metrics/compute_fad.py \
  --target-dir /path/to/reference \
  --preds-dir /path/to/reconstructions
```

## Troubleshooting

- **Missing reconstructions** – ensure the checkpoint includes
  `inference_config`. If it does not, regenerate it using
  `tools/regenerate_checkpoint.py` (see README for details).
- **CUDA not available** – use `--infer-device cpu` and adjust metric devices
  accordingly.
- **Extension mismatches** – normalise the `--extensions` list across all
  scripts to keep reference and prediction sets aligned.

For additional background on the metrics themselves, consult the inline
comments in the respective Python modules. Each script is documented and
intended to be reusable outside the Bash wrapper.
