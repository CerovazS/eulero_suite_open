# Trainer + optimization + logging + losses + seed/device
seed: 94
device: cuda

optimizer:
  _target_: torch.optim.AdamW
  lr: 5e-3
  weight_decay: 1e-4
  betas: [0.9, 0.98]

scheduler:
  _target_: ar_spectra.training_utils.utils.InverseLR
  inv_gamma: 30000
  power: 0.5
  warmup: 0.997

trainer:
  precision: 32-true
  epochs: 500
  amp: false
  log_interval: 1
  deterministic: false
  strict_deterministic: false
  ckpt_dir: checkpoints/seanet_stft
  profile: false
  check_val_every_n_epoch: 4
  save_every_n_epochs: 8
  num_sanity_val_steps: 0
  log_model_structure: true
  warmup_steps: 0
  warmup_mode: adv
  encoder_freeze_on_warmup: false
  profiler_dir: lightning_profiler
  val_check_interval: null
  strategy: auto
  num_gpus: 1
  terminate_on_nan: true
  detect_anomaly: false
  # gradient_clip_val/algorithm rimossi: non supportati con manual optimization
  # Il gradient clipping viene gestito manualmente in engine.py se necessario

wandb:
  project: harmonai_train
  name: FMA_SEANet_real
  use_wandb: true

eval_loss_config:
  sisdr:
    zero_mean: true
    reduction: mean
  stft:
    sample_rate: 44100
    fft_size: 2048
    hop_size: 512
    win_length: 2048
  mel:
    fft_size: 2048
    hop_size: 512
    win_length: 2048
    n_mels: 128

loss_config:
  spectral:
    apply_pre_transform_to_wave_losses: false
    stft_mse:
      config:
        reduction: mean
    weights: {stft_mse: 1.0}
  bottleneck:
    weights:
      kl: 1e-5
